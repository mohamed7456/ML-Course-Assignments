{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamed7456/ML-Course-Assignments/blob/main/notebooks/02_linear_regression_%26_classification_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression and Classification from Scratch**\n",
        "## Objective\n",
        "This notebook demonstrates the implementation of two fundamental machine learning algorithms from scratch using NumPy:\n",
        "1.  **Linear Regression:** To predict a continuous value by finding the line of best fit for a given set of data points.\n",
        "2.  **Binary Classification:** To classify data into one of two categories using a linear model, based on the Iris dataset."
      ],
      "metadata": {
        "id": "AyjBTZ5Nnhti"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq5nI0OQNpPu"
      },
      "source": [
        "## **Part 1 - Prediction**\n",
        "This part builds a linear regression model to find the coefficients of a line that best fits the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bKZjmrxotilS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_cH0suMwmJmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c85eee0-3333-4e01-de0c-48b01bb22e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing y = 4x + 3\n",
            "[3, 7, 11, 15, -1, -5]\n",
            "\n",
            "Testing y = 2x + 1\n",
            "[1, 3, 5, 7, -1, -3]\n",
            "\n",
            "Testing y = -1x + 5\n",
            "[5, 4, 3, 2, 6, 7]\n",
            "\n",
            "Testing y = 0x + 10\n",
            "[10, 10, 10, 10, 10, 10]\n",
            "\n",
            "Testing y = 1.5x + -2\n",
            "[-2.0, -0.5, 1.0, 2.5, -3.5, -5.0]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def linearEquation(x, a, b):\n",
        "    \"\"\"General linear equation: y = a * x + b\"\"\"\n",
        "    return a * x + b\n",
        "\n",
        "# Test cases\n",
        "x_values = [0, 1, 2, 3, -1, -2]\n",
        "test_cases = [\n",
        "    (4, 3),   # y = 4x + 3\n",
        "    (2, 1),   # y = 2x + 1\n",
        "    (-1, 5),  # y = -x + 5\n",
        "    (0, 10),  # y = 10\n",
        "    (1.5, -2) # y = 1.5x - 2\n",
        "]\n",
        "\n",
        "for a, b in test_cases:\n",
        "    print(f\"Testing y = {a}x + {b}\")\n",
        "    print([linearEquation(x, a, b) for x in x_values])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfXsKYCqEwTN",
        "outputId": "987c01e6-d06f-4509-edad-cb71c0dd4de0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.int64(11), np.int64(23), np.int64(31), np.int64(47), np.int64(63)]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "x_values = np.array([2, 5, 7, 11, 15])\n",
        "y_values = []\n",
        "for i in range(len(x_values)):\n",
        "  y_values.append(linearEquation(x_values[i], a=4, b=3))\n",
        "y_values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparing the Data: Creating the Design Matrix (X)**\n",
        "\n",
        "To solve for the model's weights (coefficients), we need to format our input data `x_values` into a **design matrix**, which we'll call `X`.\n",
        "\n",
        "According to the linear equation `y = b + ax` (or `y = w_0 * 1 + w_1 * x`), we have two coefficients to find:\n",
        "*   `w_0`: The y-intercept or bias term.\n",
        "*   `w_1`: The slope.\n",
        "\n",
        "To solve for both simultaneously, we need to add a \"dummy\" feature to our `x_values`. This dummy feature is always `1` and corresponds to the intercept `w_0`.\n",
        "\n",
        "So, for each `x_i`, our input vector becomes `[1, x_i]`. The cell below creates this matrix `X` by adding a column of ones to our `x_values`."
      ],
      "metadata": {
        "id": "FcQrfdroqaWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoyOulAsGOXZ",
        "outputId": "3defffa0-cbe8-4147-b1fa-25f48911e383"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2],\n",
              "       [ 1,  5],\n",
              "       [ 1,  7],\n",
              "       [ 1, 11],\n",
              "       [ 1, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "X = np.vstack((np.ones_like(x_values), x_values)).T\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating the Model Parameters (W) using the Normal Equation**\n",
        "---\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "The image above shows the **Normal Equation**, which is an analytical approach to finding the optimal weights (`W`) for a linear regression model. The formula is:\n",
        "\n",
        "`W = (X^T * X)^-1 * X^T * y`\n",
        "\n",
        "Where:\n",
        "- `W`: The vector of our model's weights (e.g., `[w_0, w_1]`).\n",
        "- `X`: The design matrix we just created (with the column of ones).\n",
        "- `y`: The vector of target values.\n",
        "- `X^T`: The transpose of the design matrix `X`.\n",
        "- `( ... )^-1`: The inverse of a matrix.\n",
        "\n",
        "The function `calculate_W` below implements this equation step-by-step using NumPy."
      ],
      "metadata": {
        "id": "5FmRWusXq5WT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TmWqgc6h7QI",
        "outputId": "5f613be3-4224-4018-a2d3-9712b04bd537"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "def calculate_W(X, y):\n",
        "    # Transpose the design matrix X: X^T\n",
        "    X_transposed = np.transpose(X)\n",
        "\n",
        "    # Multiply the transposed matrix by the original matrix: (X^T * X)\n",
        "    X_transposed_times_X = np.dot(X_transposed, X)\n",
        "\n",
        "    # Calculate the inverse of the result: (X^T * X)^-1\n",
        "    inverse_X_transposed_times_X = np.linalg.inv(X_transposed_times_X)\n",
        "\n",
        "    # Multiply the transposed matrix by the target vector y: (X^T * y)\n",
        "    X_transposed_times_y = np.dot(X_transposed, y)\n",
        "\n",
        "    # Finally, calculate the weights W by multiplying the two results\n",
        "    # W = (X^T * X)^-1 * (X^T * y)\n",
        "    W = np.dot(inverse_X_transposed_times_X, X_transposed_times_y)\n",
        "    return W\n",
        "\n",
        "W = calculate_W(X, y_values)\n",
        "W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l7TV3_Hlmx0",
        "outputId": "ead2f251-6081-4abb-c2cb-5a05f5597ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The original equation: y = 4x + 3\n",
            "Equation of the best fit line: y = 4.000000000000001x + 2.9999999999999933\n"
          ]
        }
      ],
      "source": [
        "print('The original equation: y = 4x + 3')\n",
        "print(f'Equation of the best fit line: y = {W[1]}x + {W[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3Xg9oJ7tQ1s",
        "outputId": "9f55fe91-ce25-462b-8b30-bfe8031b5aff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equation after adding some noise: y = 3.9666047003981983x + 3.683255506474996\n"
          ]
        }
      ],
      "source": [
        "# add some noise\n",
        "noise = np.random.rand(5)\n",
        "y_values_noisy = linearEquation(x_values, a=4, b=3) + noise\n",
        "W_noisy = calculate_W(X, y_values_noisy)\n",
        "\n",
        "print(f'Equation after adding some noise: y = {W_noisy[1]}x + {W_noisy[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLT7ecgFN_t1"
      },
      "source": [
        "## **Part 2 - Classification with the Iris Dataset**\n",
        "This part uses a linear model to solve a binary classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yG7EIQ5POEm8"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND3H7amdRLPi",
        "outputId": "ce064eee-f4f9-42b1-d725-c93f4f415960"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "y_iris"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will use a linear model to perform classification. The goal is to build a model that can distinguish between different species of Iris flowers.\n",
        "\n",
        "The Iris dataset contains 150 samples from three species (50 each):\n",
        "- `0`: Iris Setosa\n",
        "- `1`: Iris Versicolour\n",
        "- `2`: Iris Virginica\n",
        "\n",
        "For simplicity, we will convert this into a **binary classification problem**. We will train a model to distinguish **Class I (Iris Setosa)** from **Class II (everything else)**."
      ],
      "metadata": {
        "id": "bCVcQFD7rhmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Df5spPhqPjQA"
      },
      "outputs": [],
      "source": [
        "# Class I\n",
        "X_classI = X_iris[y_iris == 0]\n",
        "# Class II\n",
        "X_classII = X_iris[(y_iris == 1) | (y_iris == 2)]\n",
        "y_classII = y_iris[(y_iris == 1) | (y_iris == 2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "FtB7liNBWuNH"
      },
      "outputs": [],
      "source": [
        "# Splitting Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Class I\n",
        "X_train_classI, X_test_classI, y_train_classI, y_test_classI = train_test_split(X_classI, [0] * X_classI.shape[0], test_size = 10, random_state = 42)\n",
        "# Class II\n",
        "X_train_classII, X_test_classII, y_train_classII, y_test_classII = train_test_split(X_classII, y_classII, test_size = 20, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "M_I9DScDd3jl"
      },
      "outputs": [],
      "source": [
        "y_train_classI = np.ones((X_train_classI.shape[0],))\n",
        "y_train_classII = np.ones((X_train_classII.shape[0],))\n",
        "\n",
        "y_test_classI = np.ones((X_test_classI.shape[0],))\n",
        "y_test_classII = np.ones((X_test_classII.shape[0],))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "U40fbNo7bz7n"
      },
      "outputs": [],
      "source": [
        "X_trainClassI = np.vstack((np.ones_like(y_train_classI), X_train_classI.T)).T\n",
        "X_trainClassII = np.vstack((np.ones_like(y_train_classII), X_train_classII.T)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr4d3WyCjrG6",
        "outputId": "fd5ccad8-04bb-4020-8c27-fe3c25cd9bde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1. ,  4.8,  3. ,  1.4,  0.1],\n",
              "       [ 1. ,  5. ,  3.6,  1.4,  0.2],\n",
              "       [ 1. ,  4.9,  3.6,  1.4,  0.1],\n",
              "       [ 1. ,  4.4,  2.9,  1.4,  0.2],\n",
              "       [ 1. ,  4.6,  3.1,  1.5,  0.2],\n",
              "       [ 1. ,  4.6,  3.4,  1.4,  0.3],\n",
              "       [ 1. ,  4.5,  2.3,  1.3,  0.3],\n",
              "       [ 1. ,  5.1,  3.8,  1.6,  0.2],\n",
              "       [ 1. ,  4.6,  3.2,  1.4,  0.2],\n",
              "       [ 1. ,  5.7,  4.4,  1.5,  0.4],\n",
              "       [ 1. ,  4.9,  3.1,  1.5,  0.1],\n",
              "       [ 1. ,  5.4,  3.9,  1.3,  0.4],\n",
              "       [ 1. ,  4.8,  3.4,  1.9,  0.2],\n",
              "       [ 1. ,  4.9,  3.1,  1.5,  0.2],\n",
              "       [ 1. ,  5.4,  3.4,  1.5,  0.4],\n",
              "       [ 1. ,  5.1,  3.5,  1.4,  0.2],\n",
              "       [ 1. ,  5.1,  3.8,  1.9,  0.4],\n",
              "       [ 1. ,  5.2,  3.5,  1.5,  0.2],\n",
              "       [ 1. ,  5.5,  4.2,  1.4,  0.2],\n",
              "       [ 1. ,  5.4,  3.9,  1.7,  0.4],\n",
              "       [ 1. ,  4.7,  3.2,  1.6,  0.2],\n",
              "       [ 1. ,  4.8,  3.4,  1.6,  0.2],\n",
              "       [ 1. ,  5.5,  3.5,  1.3,  0.2],\n",
              "       [ 1. ,  4.9,  3. ,  1.4,  0.2],\n",
              "       [ 1. ,  5.1,  3.7,  1.5,  0.4],\n",
              "       [ 1. ,  4.7,  3.2,  1.3,  0.2],\n",
              "       [ 1. ,  5. ,  3.5,  1.6,  0.6],\n",
              "       [ 1. ,  5. ,  3.2,  1.2,  0.2],\n",
              "       [ 1. ,  5.1,  3.3,  1.7,  0.5],\n",
              "       [ 1. ,  5. ,  3.5,  1.3,  0.3],\n",
              "       [ 1. ,  5.4,  3.7,  1.5,  0.2],\n",
              "       [ 1. ,  4.6,  3.6,  1. ,  0.2],\n",
              "       [ 1. ,  5.7,  3.8,  1.7,  0.3],\n",
              "       [ 1. ,  5. ,  3.3,  1.4,  0.2],\n",
              "       [ 1. ,  5.4,  3.4,  1.7,  0.2],\n",
              "       [ 1. ,  5. ,  3.4,  1.5,  0.2],\n",
              "       [ 1. ,  4.4,  3.2,  1.3,  0.2],\n",
              "       [ 1. ,  5.8,  4. ,  1.2,  0.2],\n",
              "       [ 1. ,  5.2,  3.4,  1.4,  0.2],\n",
              "       [ 1. ,  4.4,  3. ,  1.3,  0.2],\n",
              "       [-1. , -7.6, -3. , -6.6, -2.1],\n",
              "       [-1. , -6. , -3. , -4.8, -1.8],\n",
              "       [-1. , -6.8, -2.8, -4.8, -1.4],\n",
              "       [-1. , -5.8, -2.6, -4. , -1.2],\n",
              "       [-1. , -6. , -2.2, -5. , -1.5],\n",
              "       [-1. , -6.7, -3.1, -4.4, -1.4],\n",
              "       [-1. , -5.5, -2.6, -4.4, -1.2],\n",
              "       [-1. , -6.3, -2.5, -5. , -1.9],\n",
              "       [-1. , -5.2, -2.7, -3.9, -1.4],\n",
              "       [-1. , -7.7, -2.8, -6.7, -2. ],\n",
              "       [-1. , -5.9, -3. , -4.2, -1.5],\n",
              "       [-1. , -6.2, -2.9, -4.3, -1.3],\n",
              "       [-1. , -7.7, -3. , -6.1, -2.3],\n",
              "       [-1. , -6. , -2.9, -4.5, -1.5],\n",
              "       [-1. , -6.8, -3.2, -5.9, -2.3],\n",
              "       [-1. , -5.7, -2.8, -4.5, -1.3],\n",
              "       [-1. , -6.5, -3. , -5.5, -1.8],\n",
              "       [-1. , -6.4, -3.2, -5.3, -2.3],\n",
              "       [-1. , -6. , -3.4, -4.5, -1.6],\n",
              "       [-1. , -5.6, -3. , -4.5, -1.5],\n",
              "       [-1. , -5.7, -2.8, -4.1, -1.3],\n",
              "       [-1. , -5.4, -3. , -4.5, -1.5],\n",
              "       [-1. , -4.9, -2.4, -3.3, -1. ],\n",
              "       [-1. , -6.7, -3. , -5.2, -2.3],\n",
              "       [-1. , -6.7, -3. , -5. , -1.7],\n",
              "       [-1. , -5.6, -2.5, -3.9, -1.1],\n",
              "       [-1. , -7.9, -3.8, -6.4, -2. ],\n",
              "       [-1. , -6.6, -3. , -4.4, -1.4],\n",
              "       [-1. , -6.8, -3. , -5.5, -2.1],\n",
              "       [-1. , -6.1, -2.9, -4.7, -1.4],\n",
              "       [-1. , -6.4, -2.9, -4.3, -1.3],\n",
              "       [-1. , -5.5, -2.3, -4. , -1.3],\n",
              "       [-1. , -5.8, -2.7, -4.1, -1. ],\n",
              "       [-1. , -5.6, -3. , -4.1, -1.3],\n",
              "       [-1. , -6.6, -2.9, -4.6, -1.3],\n",
              "       [-1. , -6.4, -2.8, -5.6, -2.1],\n",
              "       [-1. , -6.3, -3.3, -4.7, -1.6],\n",
              "       [-1. , -5.8, -2.8, -5.1, -2.4],\n",
              "       [-1. , -6.7, -3.1, -4.7, -1.5],\n",
              "       [-1. , -6.9, -3.1, -5.4, -2.1],\n",
              "       [-1. , -4.9, -2.5, -4.5, -1.7],\n",
              "       [-1. , -5.9, -3. , -5.1, -1.8],\n",
              "       [-1. , -6.5, -3. , -5.8, -2.2],\n",
              "       [-1. , -5. , -2.3, -3.3, -1. ],\n",
              "       [-1. , -6.3, -3.3, -6. , -2.5],\n",
              "       [-1. , -7.7, -3.8, -6.7, -2.2],\n",
              "       [-1. , -5.7, -2.9, -4.2, -1.3],\n",
              "       [-1. , -7.7, -2.6, -6.9, -2.3],\n",
              "       [-1. , -6.4, -2.7, -5.3, -1.9],\n",
              "       [-1. , -6.5, -3. , -5.2, -2. ],\n",
              "       [-1. , -7.2, -3. , -5.8, -1.6],\n",
              "       [-1. , -6.1, -3. , -4.6, -1.4],\n",
              "       [-1. , -6.7, -2.5, -5.8, -1.8],\n",
              "       [-1. , -5.1, -2.5, -3. , -1.1],\n",
              "       [-1. , -6.2, -3.4, -5.4, -2.3],\n",
              "       [-1. , -7.3, -2.9, -6.3, -1.8],\n",
              "       [-1. , -7.2, -3.2, -6. , -1.8],\n",
              "       [-1. , -5.8, -2.7, -3.9, -1.2],\n",
              "       [-1. , -6.7, -3.3, -5.7, -2.5],\n",
              "       [-1. , -7.2, -3.6, -6.1, -2.5],\n",
              "       [-1. , -5.7, -2.5, -5. , -2. ],\n",
              "       [-1. , -6.1, -2.6, -5.6, -1.4],\n",
              "       [-1. , -6.3, -2.3, -4.4, -1.3],\n",
              "       [-1. , -5.7, -2.6, -3.5, -1. ],\n",
              "       [-1. , -6.4, -3.2, -4.5, -1.5],\n",
              "       [-1. , -7.1, -3. , -5.9, -2.1],\n",
              "       [-1. , -6.1, -2.8, -4. , -1.3],\n",
              "       [-1. , -6.9, -3.1, -4.9, -1.5],\n",
              "       [-1. , -6.1, -2.8, -4.7, -1.2],\n",
              "       [-1. , -6.4, -3.1, -5.5, -1.8],\n",
              "       [-1. , -6.9, -3.1, -5.1, -2.3],\n",
              "       [-1. , -6.7, -3.3, -5.7, -2.1],\n",
              "       [-1. , -6.3, -3.4, -5.6, -2.4],\n",
              "       [-1. , -6.4, -2.8, -5.6, -2.2],\n",
              "       [-1. , -5.9, -3.2, -4.8, -1.8],\n",
              "       [-1. , -6.5, -3.2, -5.1, -2. ],\n",
              "       [-1. , -5.6, -2.8, -4.9, -2. ],\n",
              "       [-1. , -5.6, -2.9, -3.6, -1.3],\n",
              "       [-1. , -5.8, -2.7, -5.1, -1.9],\n",
              "       [-1. , -5.8, -2.7, -5.1, -1.9]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "X_train = np.concatenate((X_trainClassI, -1 * X_trainClassII))\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Preprocessing: One-vs-Rest Strategy**\n",
        "\n",
        "We are using a **One-vs-Rest (OvR)** approach. We've defined two groups:\n",
        "- **Class I:** All samples where `y_iris == 0` (Setosa).\n",
        "- **Class II:** All other samples (`y_iris == 1` or `y_iris == 2`).\n",
        "\n",
        "The goal is to find a decision boundary (a line or hyperplane) that separates Class I from Class II."
      ],
      "metadata": {
        "id": "kmjWshM5rzB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparing Data for Linear Classification**\n",
        "\n",
        "To use our linear regression solver for classification, we use a clever trick. We want to find a weight vector `W` such that the sign of `W.X` tells us the class of a sample.\n",
        "\n",
        "- For **Class I** samples (`X_trainClassI`), we want `W.X > 0`.\n",
        "- For **Class II** samples (`X_trainClassII`), we want `W.X < 0`.\n",
        "\n",
        "We can achieve this by creating a new training set `X_train` where the feature values of all Class II samples are multiplied by -1. Then, we can train the model to make all outputs positive.\n",
        "\n",
        "The target `y` will simply be a vector of ones. The `calculate_weights` function will then find a `W` that tries to satisfy `W * X_train > 0` for all samples."
      ],
      "metadata": {
        "id": "K0COYkf2sSIh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmD7W923zfF8",
        "outputId": "12938a42-c82a-4421-b84e-41bf5a3ab56c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.3,  3. ,  1.1,  0.1],\n",
              "       [ 5.1,  3.4,  1.5,  0.2],\n",
              "       [ 4.8,  3.1,  1.6,  0.2],\n",
              "       [ 4.8,  3. ,  1.4,  0.3],\n",
              "       [ 5.1,  3.5,  1.4,  0.3],\n",
              "       [ 5.3,  3.7,  1.5,  0.2],\n",
              "       [ 5. ,  3.4,  1.6,  0.4],\n",
              "       [ 5. ,  3. ,  1.6,  0.2],\n",
              "       [ 5.2,  4.1,  1.5,  0.1],\n",
              "       [ 5.1,  3.8,  1.5,  0.3],\n",
              "       [-6.3, -2.8, -5.1, -1.5],\n",
              "       [-6.3, -2.9, -5.6, -1.8],\n",
              "       [-6.9, -3.2, -5.7, -2.3],\n",
              "       [-5.7, -3. , -4.2, -1.2],\n",
              "       [-5.6, -2.7, -4.2, -1.3],\n",
              "       [-5.5, -2.5, -4. , -1.3],\n",
              "       [-6.3, -2.5, -4.9, -1.5],\n",
              "       [-7.4, -2.8, -6.1, -1.9],\n",
              "       [-5. , -2. , -3.5, -1. ],\n",
              "       [-7. , -3.2, -4.7, -1.4],\n",
              "       [-6.2, -2.2, -4.5, -1.5],\n",
              "       [-5.5, -2.4, -3.8, -1.1],\n",
              "       [-6.3, -2.7, -4.9, -1.8],\n",
              "       [-6. , -2.7, -5.1, -1.6],\n",
              "       [-6.7, -3.1, -5.6, -2.4],\n",
              "       [-6.5, -2.8, -4.6, -1.5],\n",
              "       [-6.2, -2.8, -4.8, -1.8],\n",
              "       [-6.1, -3. , -4.9, -1.8],\n",
              "       [-6. , -2.2, -4. , -1. ],\n",
              "       [-5.5, -2.4, -3.7, -1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "X_test = np.vstack((X_test_classI, -1 * X_test_classII))\n",
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYQeMK0vhzxx",
        "outputId": "ecdac930-5044-40f8-e7a1-b7ac6deeeb3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "       1.])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "y_train = np.concatenate((y_train_classI, y_train_classII), axis=0)\n",
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A10Tr7UBoHWm"
      },
      "source": [
        "### **Calculating the Weight Vector (W)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "---\n",
        "\n",
        "The `calculate_weights` function below is identical to the one we used for regression. It solves the normal equation `W = (Z^T * Z)^-1 * Z^T * y`.\n",
        "\n",
        "However, its purpose here is different:\n",
        "- **`Z`**: Our modified training data (`X_train`), where Class II features have been negated.\n",
        "- **`y`**: A vector of ones.\n",
        "- **`W`**: The resulting weight vector is not a line of best fit, but the normal vector to the **decision boundary hyperplane** that separates the two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjFRYg82mwub",
        "outputId": "aeea35f5-eceb-4556-b427-36335f334331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.72396059,  0.12016826,  0.47807312, -0.41859655, -0.16621896])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "def calculate_weights(X_train, y_train):\n",
        "    Z = X_train\n",
        "    Z_transpose = Z.T\n",
        "    Z_transpose_Z = np.dot(Z_transpose, Z)\n",
        "    Z_transpose_Z_inv = np.linalg.inv(Z_transpose_Z)\n",
        "    Z_transpose_Z_inv_Z_transpose = np.dot(Z_transpose_Z_inv, Z_transpose)\n",
        "\n",
        "    W = np.dot(Z_transpose_Z_inv_Z_transpose, y_train)\n",
        "\n",
        "    return W\n",
        "\n",
        "W2 = calculate_weights(X_train, y_train)\n",
        "W2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY6gJrfIG76E",
        "outputId": "caa6cf98-1232-4c7d-da20-e40f1e092b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original classes: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Predicted classes: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "def check_classes(X_train, W, y_train):\n",
        "    y_pred_train = np.sign(X_train @ W)\n",
        "\n",
        "    return y_pred_train, y_train\n",
        "\n",
        "y_pred_train, y_train_original = check_classes(X_train, W2, y_train)\n",
        "print(\"Original classes:\", y_train_original)\n",
        "print(\"Predicted classes:\", y_pred_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYFrQTbCH-pq",
        "outputId": "8b2939a6-814f-4419-bce5-54af9f8d9a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check class of training data:\n",
            "Matching samples: 120\n",
            "Total samples: 120\n",
            "Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "y_pred_train, y_train_original = check_classes(X_train, W2, y_train)\n",
        "\n",
        "def accuracy(y_pred, y_original):\n",
        "  matching = np.sum(y_pred == y_original)\n",
        "  total_samples = len(y_pred)\n",
        "  accuracy = matching / total_samples\n",
        "\n",
        "  print(\"Matching samples:\", matching)\n",
        "  print(\"Total samples:\", total_samples)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "\n",
        "print(\"Check class of training data:\")\n",
        "accuracy(y_pred_train, y_train_original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZs5IGsFoxHt",
        "outputId": "89c5b033-3718-455b-c47b-a8014d1dc4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:\n",
            "Sample 0 belongs to ClassI\n",
            "Sample 1 belongs to ClassI\n",
            "Sample 2 belongs to ClassI\n",
            "Sample 3 belongs to ClassI\n",
            "Sample 4 belongs to ClassI\n",
            "Sample 5 belongs to ClassI\n",
            "Sample 6 belongs to ClassI\n",
            "Sample 7 belongs to ClassI\n",
            "Sample 8 belongs to ClassI\n",
            "Sample 9 belongs to ClassI\n",
            "Sample 10 belongs to ClassII\n",
            "Sample 11 belongs to ClassII\n",
            "Sample 12 belongs to ClassII\n",
            "Sample 13 belongs to ClassII\n",
            "Sample 14 belongs to ClassII\n",
            "Sample 15 belongs to ClassII\n",
            "Sample 16 belongs to ClassII\n",
            "Sample 17 belongs to ClassII\n",
            "Sample 18 belongs to ClassII\n",
            "Sample 19 belongs to ClassII\n",
            "Sample 20 belongs to ClassII\n",
            "Sample 21 belongs to ClassII\n",
            "Sample 22 belongs to ClassII\n",
            "Sample 23 belongs to ClassII\n",
            "Sample 24 belongs to ClassII\n",
            "Sample 25 belongs to ClassII\n",
            "Sample 26 belongs to ClassII\n",
            "Sample 27 belongs to ClassII\n",
            "Sample 28 belongs to ClassII\n",
            "Sample 29 belongs to ClassII\n"
          ]
        }
      ],
      "source": [
        "def predict(X_test, W):\n",
        "    # First, add the bias term (a column of ones) to the test data.\n",
        "    # This is crucial, as the model was trained on data with a bias term.\n",
        "    X_test_bias = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "    # Calculate the dot product of the test data and the weight vector.\n",
        "    # The sign of the result determines the predicted class.\n",
        "    # np.sign() returns +1 if the value is positive, and -1 if it's negative.\n",
        "    y_pred_test = np.sign(X_test_bias @ W)\n",
        "\n",
        "    return y_pred_test\n",
        "\n",
        "y_pred_test = predict(X_test, W2)\n",
        "print(\"Predicted classes:\")\n",
        "for i, x in enumerate(y_pred_test):\n",
        "  if x == 1:\n",
        "    print(f\"Sample {i} belongs to ClassI\")\n",
        "  elif x == -1:\n",
        "    print(f\"Sample {i} belongs to ClassII\")\n",
        "  else:\n",
        "    print(f\"Sample {i} Undefined\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}